{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOaWUUpYHgaojV92S0yXu2i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielFadlon/LWSM/blob/main/LWSM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LWSM (Light-Weight Signature Matching)**\n",
        "A framework for a resource limited service using PDS (Probabilistic Data Structure) and AI powered methods."
      ],
      "metadata": {
        "id": "FdOfkhSt54zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_project = 'drive/MyDrive/AIForCyber/LWSM'\n",
        "# Define the ember2018 directory path\n",
        "features_path = f'{path_to_project}/ember2018'\n",
        "# Define a path to save the vectorized features (to prevent from creating them each time)\n",
        "vectorized_path = f'{path_to_project}/ember_vectorized.npy'\n",
        "\n",
        "# Define the given initial blacklist file path\n",
        "file_path_to_malicous_sha256_ember = f'{path_to_project}/task1_malicous_sha256_ember.txt'"
      ],
      "metadata": {
        "id": "eLSElnU8YYf0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/zivido/ember-3.9.git\n",
        "!pip install lief\n",
        "!pip install bitarray"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivsZ9yVI9gNk",
        "outputId": "b203afd4-91e7-40a7-b9a0-dfe2bf188aba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/zivido/ember-3.9.git\n",
            "  Cloning https://github.com/zivido/ember-3.9.git to /tmp/pip-req-build-h42a7rq4\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/zivido/ember-3.9.git /tmp/pip-req-build-h42a7rq4\n",
            "  Resolved https://github.com/zivido/ember-3.9.git to commit bc8fe8fe990ed172c69320d13b757b384b8cc367\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ember\n",
            "  Building wheel for ember (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ember: filename=ember-0.1.0-py3-none-any.whl size=13096 sha256=f7d254ef7200b20eb6cfcb3ebc26a0d86497359d5b077e4265a42a31b8e6c470\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jholkkjq/wheels/8a/de/4f/e2eadc2237ea0e00160b8a5b067e4e2d8ef94f361258d38dcf\n",
            "Successfully built ember\n",
            "Installing collected packages: ember\n",
            "Successfully installed ember-0.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting lief\n",
            "  Downloading lief-0.13.0-cp310-cp310-manylinux_2_24_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lief\n",
            "Successfully installed lief-0.13.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-2.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.7/272.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitarray\n",
            "Successfully installed bitarray-2.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import ember\n",
        "import math\n",
        "import numpy as np\n",
        "import hashlib\n",
        "import bitarray"
      ],
      "metadata": {
        "id": "hb3RaSq99XUz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wY7yIkbmJ2gv",
        "outputId": "53222970-28c1-4b20-a353-ed83551e0ed2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **EMBER Data Set**\n",
        "\n",
        "- Investigate the data set.\n",
        "- Bla Bla ... Say more about wht we are doing in this part "
      ],
      "metadata": {
        "id": "NNH41Rc08cOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the vectorized features have already been saved\n",
        "try:\n",
        "    X_vectorized = np.load(vectorized_path)\n",
        "    print('Loaded vectorized features from file')\n",
        "except FileNotFoundError:\n",
        "    # If the file doesn't exist, create the vectorized features and save them to a file\n",
        "    X_vectorized = ember.create_vectorized_features(features_path)\n",
        "    np.save(vectorized_path, X_vectorized)\n",
        "    print('Created and saved vectorized features to file')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQT0cyw5sJgq",
        "outputId": "9089a756-d7f4-4950-c6c5-fd22f1358ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: Custom version of EMBER being in use\n",
            "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
            "WARNING:   lief version 0.13.0-966a66c7 found instead. There may be slight inconsistencies\n",
            "WARNING:   in the feature calculations.\n",
            "Vectorizing training set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 34716/800000 [56:42<2:07:28, 100.05it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PDS**\n",
        "\n",
        "First, we will create our Probabilistic Data Structure that mapping th SHA256 codes to mlicious or binain such that: \\\n",
        "1. Each Malicious file is mapped as mlicious. \\\n",
        "    **FNR(False Negative Rate)=0**\n",
        "2. The number of binain files that are mapped as malicious is less than 0.01% from the number of all files. \\\n",
        "    **FPR(False Positive Rate)<=0.01%**"
      ],
      "metadata": {
        "id": "nUIfU_0s6RxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Bloom Filter**\n",
        "\n",
        "We are using the Bloom FIlter Data Structure for checking the exsiting of a malicious file in efficiet way. \n",
        "\n",
        "A Bloom filter is a probabilistic data structure that is used to efficiently test whether an element is a member of a set. It uses a bit array and a set of hash functions to represent the set of elements and to determine whether an element is likely to be a member of the set.\n",
        "\n",
        "Bloom filters have several advantages over other data structures such as hash tables or binary search trees. \\\n",
        "- They are very space-efficient, requiring only a small amount of memory to store the bit array and hash functions.\\\n",
        "- They are very fast, with constant-time insertions and lookups. \n",
        "- They can handle large sets of elements with a low probability of false positives.\n",
        "\n",
        "Bloom filters are particularly useful in cases where the cost of false positives is low (i.e., it is acceptable to occasionally say that a file is a malicous while is not), but the cost of false negatives is high (i.e., it is not acceptable to miss a malicous file)."
      ],
      "metadata": {
        "id": "vpXrSCax5_7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BloomFilter:\n",
        "    def __init__(self, n, fpr):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "         n - number of elements that might be inserted to the bloom filter\n",
        "         fpr - the required False Positive Rate\n",
        "        \"\"\"\n",
        "        bitarry_size, hash_count = self.find_optimal_size(n, fpr)\n",
        "        self.n = n\n",
        "        self.size = bitarry_size\n",
        "        self.hash_count = hash_count\n",
        "        self.bitarray = bitarray.bitarray(bitarry_size)\n",
        "        self.bitarray.setall(0)\n",
        "\n",
        "    def add(self, item):\n",
        "        for i in range(self.hash_count):\n",
        "            digest = hashlib.sha256(str(item).encode('utf-8') + str(i).encode('utf-8')).hexdigest()\n",
        "            index = int(digest, 16) % self.size\n",
        "            self.bitarray[index] = 1\n",
        "\n",
        "    def __contains__(self, item):\n",
        "        for i in range(self.hash_count):\n",
        "            digest = hashlib.sha256(str(item).encode('utf-8') + str(i).encode('utf-8')).hexdigest()\n",
        "            index = int(digest, 16) % self.size\n",
        "            if not self.bitarray[index]:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def get_size(self):\n",
        "      return self.size\n",
        "\n",
        "    def get_hash_count(self):\n",
        "      return self.hash_count\n",
        "\n",
        "    def find_optimal_size(self, n, fpr):\n",
        "        \"\"\"\n",
        "        The find_optimal_size function compute the optimal size of a bit-array \n",
        "        and the number of hash functions needed for a Bloom filter that will store n items with a desired false positive rate of fpr.\n",
        "        \"\"\"\n",
        "        bitarray_size = int(-1 * ((n * np.log(fpr)) / np.log(2) ** 2)) + 1\n",
        "        number_of_hash_functions = int((bitarray_size / n) * math.log(2))\n",
        "        return bitarray_size, number_of_hash_functions\n",
        "\n",
        "    def __str__(self):\n",
        "      return f\"Bloom filter size: {self.size} \\nFalse Positive Rate: {(1 - math.exp(-self.hash_count * self.n/ self.size)) ** self.hash_count} \\nNumber of Hash functions: {self.hash_count}\""
      ],
      "metadata": {
        "id": "YmpSz6Jm52wR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Malicous SHA256 for EMBER\n",
        "\n",
        "Read the SHA256 from the 'task1_malicious_sha256_ember.txt' and insert to our bloom filter."
      ],
      "metadata": {
        "id": "gkIPq1AAHKCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fpr = 0.0001\n",
        "with open(file_path_to_malicous_sha256_ember, 'r') as f:\n",
        "    n = sum(1 for line in f) \n",
        "    bloom_filter = BloomFilter(n, fpr)\n",
        "    for hash_code_line in f:\n",
        "        bloom_filter.add(hash_code_line)\n",
        "    print(bloom_filter)       "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK6Wi0LCHrsI",
        "outputId": "54919235-5951-45d1-de61-dee94cc9fe59"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bloom filter size: 7668047 \n",
            "False Positive Rate: 0.00010013457033664568 \n",
            "Number of Hash functions: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **AI Powered Methods**\n",
        "\n",
        "In the following part, we will investigate the features and try to build the best possible Model for the clients needs.\n",
        "\n",
        "There are three main issues that we are going to consider:\n",
        "1. The importance "
      ],
      "metadata": {
        "id": "MAEuwNQ37rch"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PqtbtjXt9F5L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}